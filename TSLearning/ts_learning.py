# -*- coding: utf-8 -*-
"""TS_Learning.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1yVdZarfYHd6ME8GT-WFaNzTH1d2GV16J
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
from torch.utils.data import DataLoader
import torchvision
import torchvision.transforms as tfs
import numpy as np
import matplotlib.pyplot as plt
from tqdm import trange

if torch.cuda.is_available():
  device = 'cuda'
else:
  device = 'cpu'
print(device)

batch_size = 32

translist = [tfs.ToTensor(), tfs.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))]
transforms = tfs.Compose(translist)

trainset = torchvision.datasets.STL10(root='./data', split='train', transform=transforms, download=True)
testset = torchvision.datasets.STL10(root='./data', split='test', transform=transforms, download=True)

trainloader = DataLoader(trainset, batch_size=batch_size, shuffle=True)
testloader = DataLoader(testset, batch_size=1, shuffle=False)

def imshow(img):
    img = img * 0.5 + 0.5     # unnormalize
    npimg = img.numpy()
    plt.imshow(np.transpose(npimg, (1, 2, 0)))
    plt.show()

for step, data in enumerate(trainloader):
  x, y = data
  print(x[0].shape)
  imshow(x[0])
  print(y)
  break

class TeacherNet(nn.Module):
  
  def __init__(self, ouput_size):
    super(TeacherNet, self).__init__()

    self._output_size = ouput_size

    self._cnn = nn.Sequential(
        nn.Conv2d(3, 9, kernel_size=3, padding=1),
        nn.ReLU(),
        nn.Conv2d(9, 9, kernel_size=3, padding=1),
        nn.ReLU(),
        nn.MaxPool2d(2, 2),
        #9 x 48 x 48

        nn.Conv2d(9, 15, kernel_size=3, padding=1),
        nn.BatchNorm2d(15),
        nn.ReLU(),
        nn.Conv2d(15, 15, kernel_size=3, padding=1),
        nn.ReLU(),
        nn.MaxPool2d(2, 2),
        #15 x 24 x 24

        nn.Conv2d(15, 21, kernel_size=3, padding=1),
        nn.ReLU(),
        nn.Conv2d(21, 21, kernel_size=3, padding=1),
        nn.ReLU(),
        nn.MaxPool2d(2, 2),
        #21 x 12 x 12

        nn.Conv2d(21, 27, kernel_size=3, padding=1),
        nn.BatchNorm2d(27),
        nn.ReLU(),
        nn.Conv2d(27, 27, kernel_size=3, padding=1),
        nn.ReLU(),
        nn.MaxPool2d(2, 2),
        #27 x 6 x 6

        nn.Conv2d(27, 36, kernel_size=3, padding=1),
        nn.ReLU(),
        nn.Conv2d(36, 36, kernel_size=3, padding=1),
        nn.ReLU(),
        nn.MaxPool2d(2, 2)
        #36 x 3 x 3
    )
    self._fcc = nn.Sequential(
        nn.Linear(324, 256),
        nn.ReLU(),
        nn.Linear(256, 64),
        nn.ReLU(),
        nn.Linear(64, self._output_size)
    )

  def forward(self, x):
    feature = self._cnn(x)
    feature = feature.view(feature.size(0), -1)
    y = self._fcc(feature)
    return y

class StudentNet(nn.Module):

  def __init__(self, output_size):
    super(StudentNet, self).__init__()

    self._output_size = output_size

    self._cnn = nn.Sequential(
        nn.Conv2d(3, 6, kernel_size=3, padding=1),
        nn.ReLU(),
        nn.MaxPool2d(2, 2),
        #6 x 48 x 48

        nn.Conv2d(6, 12, kernel_size=3, padding=1),
        nn.BatchNorm2d(12),
        nn.ReLU(),
        nn.MaxPool2d(2, 2),
        #12 x 24 x 24

        nn.Conv2d(12, 15, kernel_size=3, padding=1),
        nn.ReLU(),
        nn.MaxPool2d(2, 2),
        #15 x 12 x 12

        nn.Conv2d(15, 18, kernel_size=3, padding=1),
        nn.BatchNorm2d(18),
        nn.ReLU(),
        nn.MaxPool2d(2, 2),
        #18 x 6 x 6

        nn.Conv2d(18, 24, kernel_size=3, padding=1),
        nn.ReLU(),
        nn.MaxPool2d(2, 2),
        #24 x 3 x 3
    )
    self._fcc = nn.Sequential(
        nn.Linear(216, 128),
        nn.ReLU(),
        nn.Linear(128, 32),
        nn.ReLU(),
        nn.Linear(32, self._output_size)
    )

  def forward(self, x):
    feature = self._cnn(x)
    feature = feature.view(feature.size(0), -1)
    y = self._fcc(feature)
    return y

def SoftCrossEntropy(y_, y):
  log_likelihood = -F.log_softmax(y_, dim=1)
  batch_num, class_num = y.shape
  loss = torch.sum(torch.mul(log_likelihood, y_)) / batch_num
  return loss

def train_teacher(tnet, trainloader, lr, epochs, device):
  tnet.to(device)
  optimizer = optim.Adam(tnet.parameters(), lr=lr)
  loss_fn = nn.CrossEntropyLoss()

  tnet.train()
  for epoch in trange(epochs):
    losses = []
    for step, data in enumerate(trainloader):
      x, y = data
      x = x.to(device)
      y = y.to(device)

      y_ = tnet(x)
      loss = loss_fn(y_, y)
      optimizer.zero_grad()
      loss.backward()
      optimizer.step()

      losses.append(loss.item())
    
    avg_loss = sum(losses) / len(losses)
    print(f'epoch {epoch} finished============> average loss = {avg_loss}')

def train_student(tnet, snet, weights, trainloader, lr, epochs, device):
  tnet.to(device)
  snet.to(device)
  weights = weights.to(device)

  optimizer = optim.Adam(snet.parameters(), lr=lr)
  loss_fn1 = nn.CrossEntropyLoss()
  loss_fn2 = nn.KLDivLoss()
  softmax = nn.Softmax(dim=1)
  log_softmax = nn.LogSoftmax(dim=1)

  snet.train()
  tnet.eval()
  for epoch in trange(epochs):
    losses = []
    for step, data in enumerate(trainloader):
      x, y = data
      x = x.to(device)
      y = y.to(device)

      y_ = snet(x)
      y_soft_ = softmax(y_)

      y_soft = tnet(x)
      y_soft = log_softmax(y_soft)

      loss = weights[0] * loss_fn1(y_, y) + weights[1] * loss_fn2(y_soft_, y_soft)
      optimizer.zero_grad()
      loss.backward()
      optimizer.step()

      losses.append(loss.item())
    
    avg_loss = sum(losses) / len(losses)
    print(f'epoch {epoch} finished============> average loss = {avg_loss}')

def test(net, testloader, device):
  correct = 0
  total = 0
  net.eval()
  for step, data in enumerate(testloader):
    x, y = data
    x = x.to(device)
    y = y.to(device)

    y_ = net(x)
    if torch.eq(torch.argmax(y_, dim=1), y):
      correct += 1
    total += 1
  
  acc = correct / total
  print(acc)

tnet = TeacherNet(10)

train_teacher(tnet, trainloader, 0.6e-3, 35, device)
torch.save(tnet.state_dict(), 'tnet_weights.pt')

test(tnet, testloader, device)

snet = StudentNet(10)
weights = torch.tensor([0.1, 0.9])

train_student(tnet, snet, weights, trainloader, 1e-3, 20, device=device)
torch.save(snet.state_dict(), 'snet_weights.pt')

test(snet, testloader, device)

